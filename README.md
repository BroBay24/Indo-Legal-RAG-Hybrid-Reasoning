# RAG Pipeline Hukum Indonesia ğŸ‡®ğŸ‡©âš–ï¸

Pipeline Retrieval-Augmented Generation (RAG) untuk dokumen hukum Indonesia dengan **Hybrid Retriever** (BM25 + Pinecone semantic search) dan Reciprocal Rank Fusion (RRF).

## ğŸ—ï¸ Arsitektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER QUERY                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HYBRID RETRIEVER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   BM25 (Lokal)  â”‚            â”‚  Pinecone (Cloud)       â”‚    â”‚
â”‚  â”‚   - Exact Match â”‚            â”‚  - Semantic Search      â”‚    â”‚
â”‚  â”‚   - Pasal/Nomor â”‚            â”‚  - BGE-M3 Embeddings    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                  â”‚                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚           â”‚  RRF Fusion          â”‚                             â”‚
â”‚           â”‚  (Rank Aggregation)  â”‚                             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONTEXT BUILDER                             â”‚
â”‚         Top-K chunks + Metadata + Sources                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM (Llama-3 Indo GGUF)                        â”‚
â”‚         Legal Prompt Template â†’ Answer Generation               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RESPONSE                                    â”‚
â”‚         Jawaban + Sumber Dokumen                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Struktur Proyek

```
proyekrag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py          # Konfigurasi utama
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_loader.py   # PDF loader
â”‚   â”‚   â”œâ”€â”€ legal_preprocessor.py # Normalisasi teks hukum
â”‚   â”‚   â”œâ”€â”€ chunker.py           # Text chunking dengan overlap
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # BGE embedding model
â”‚   â”‚   â”œâ”€â”€ bm25_indexer.py      # BM25 index lokal
â”‚   â”‚   â”œâ”€â”€ pinecone_indexer.py  # Pinecone vector store
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py  # Hybrid search + RRF
â”‚   â”‚   â”œâ”€â”€ llm_wrapper.py       # LLM wrapper (local/cloud)
â”‚   â”‚   â”œâ”€â”€ legal_prompts.py     # Prompt templates
â”‚   â”‚   â””â”€â”€ rag_pipeline.py      # Orkestrasi pipeline
â”‚   â”œâ”€â”€ main.py                  # FastAPI application
â”‚   â”œâ”€â”€ run.py                   # CLI runner
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ *.pdf                    # Dokumen PDF sumber
â”‚   â”œâ”€â”€ processed/               # Metadata chunks
â”‚   â””â”€â”€ indices/                 # BM25 index files
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama-3-indo.gguf        # Model LLM lokal
â””â”€â”€ frontend/                    # (Coming soon)
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
cd proyekrag/backend

# Buat virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# atau: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Konfigurasi

Copy `.env.example` ke `.env` dan sesuaikan:

```bash
cp ../.env.example ../.env
```

Edit `.env`:
```env
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=hukum-rag
EMBEDDING_MODEL_NAME=BAAI/bge-m3
```

### 3. Indexing Dokumen

```bash
# Via CLI
python run.py index

# Atau tanpa Pinecone (BM25 only)
python run.py index --no-pinecone
```

### 4. Jalankan Server

```bash
# Via CLI
python run.py serve

# Atau langsung
python main.py

# Dengan reload untuk development
python run.py serve --reload
```

Server akan berjalan di `http://localhost:8000`

### 5. Test Query

```bash
# Via CLI
python run.py query "Apa putusan dalam kasus ini?"

# Interactive chat
python run.py chat

# Via API
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"pertanyaan": "Apa putusan dalam kasus ini?"}'
```

## ğŸ”Œ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/health` | GET | Detailed health status |
| `/chat` | POST | Chat dengan RAG |
| `/chat-stream` | POST | Streaming chat |
| `/chat-basic` | POST | Chat tanpa RAG |
| `/index` | POST | Index dokumen |
| `/stats` | GET | Pipeline statistics |
| `/search` | GET | Search only (debug) |
| `/clear-index` | POST | Clear semua index |

### Contoh Request

```json
POST /chat
{
  "pertanyaan": "Siapa penggugat dalam kasus ini?",
  "top_k": 5,
  "max_tokens": 512,
  "temperature": 0.7,
  "include_context": true
}
```

### Contoh Response

```json
{
  "jawaban": "Berdasarkan dokumen, penggugat dalam kasus ini adalah...",
  "sumber": [
    {
      "source": "putusan_690_pdt.g_2024.pdf",
      "page": 1,
      "doc_type": "putusan",
      "score": 0.85
    }
  ],
  "konteks": "[Sumber 1: ...]",
  "pertanyaan": "Siapa penggugat dalam kasus ini?"
}
```

## âš™ï¸ Konfigurasi

Semua konfigurasi ada di `config/settings.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `CHUNK_SIZE` | 1000 | Ukuran chunk |
| `CHUNK_OVERLAP` | 200 | Overlap antar chunk |
| `BM25_TOP_K` | 10 | Top-K BM25 |
| `SEMANTIC_TOP_K` | 10 | Top-K Pinecone |
| `FINAL_TOP_K` | 5 | Final results |
| `FUSION_METHOD` | "rrf" | rrf/weighted/interleave |
| `RRF_K` | 60 | RRF constant |
| `LLM_MAX_TOKENS` | 512 | Max LLM tokens |
| `LLM_TEMPERATURE` | 0.7 | LLM temperature |

## ğŸ§  Model

- **Embedding**: `BAAI/bge-m3` (atau `paraphrase-multilingual-MiniLM-L12-v2`)
- **LLM**: Llama-3 Indo (GGUF format)
- **Vector Store**: Pinecone (cloud)
- **Lexical Search**: BM25Okapi (lokal)

## ğŸ“ License

MIT License
